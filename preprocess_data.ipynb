{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16118e2",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3980be",
   "metadata": {},
   "source": [
    "This notebook is set up to preprocess the data. While preprocessing, the data will be visualized for further understanding. The goal is to clean and understand the data to ultimately set it up for feature selection and training under models.\n",
    "\n",
    "The preprocessing steps contain:\n",
    "- log transformation and standardization of numerical data, and \n",
    "- encoding categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f31eb",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import mstats\n",
    "from sklearn.base import TransformerMixin\n",
    "import numpy.ma as ma\n",
    "\n",
    "# changed display options to accomodate for long list in output\n",
    "# pd.set_option('display.max_rows', 1000)\n",
    "# pd.set_option('display.max_columns', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c155616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can load ~1/6 of dataset so will split into 3 chunks\n",
    "n_chunks=6\n",
    "\n",
    "# total num of rows in OG file\n",
    "n_rows=sum(1 for row in open('train.csv')) -1 # subtract header row\n",
    "\n",
    "chunk_size=n_rows // n_chunks\n",
    "\n",
    "chunk = next(pd.read_csv('train.csv', chunksize=chunk_size, low_memory=False))\n",
    "print(f'chunk shape - before : {chunk.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "# drop col with NA > than threshold (50%)\n",
    "NA_thresh = 0.5\n",
    "# outlier threshold\n",
    "out_thresh=3\n",
    "\n",
    "# define features\n",
    "cat_col=chunk.select_dtypes(include='object').columns\n",
    "time_col=['VAR_0073','VAR_0075','VAR_0156','VAR_0157',\n",
    "          'VAR_0158','VAR_0159','VAR_0166','VAR_0167','VAR_0168','VAR_0169',\n",
    "           'VAR_0176','VAR_0177','VAR_0178','VAR_0179','VAR_0204','VAR_0217','VAR_0314','VAR_0531']\n",
    "num_col=chunk.select_dtypes(include=np.number).columns.difference(time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_ck=Prep(chunk,time_col)\n",
    "prepped_ck=processor_ck.process_df()\n",
    "\n",
    "print(f'Chunk shape - after: {prepped_ck.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables \n",
    "cat_col=prepped_ck.select_dtypes(include='object').columns\n",
    "num_col=prepped_ck.select_dtypes(include=np.number).columns.difference(time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_ck=NA(prepped_ck,NA_thresh,num_col,cat_col)\n",
    "\n",
    "# initial NA count\n",
    "print(f'Number of NA - before: {np.any(prepped_ck.isnull())}')\n",
    "\n",
    "# apply NA function\n",
    "imputed_ck = processor_ck.process_na()\n",
    "\n",
    "# post NA count\n",
    "print(f'Number of NA - after: {np.any(imputed_ck.isnull())}')\n",
    "print(f'Chunk shape - after: {imputed_ck.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f8c77",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfe187",
   "metadata": {},
   "source": [
    "### 2.1 transforming numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99f026",
   "metadata": {},
   "source": [
    "Will apply:\n",
    "1. Log Transformation - makes initial data distribution more normal and handles skewed data\n",
    "2. Standardization - ensures all features are on the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121a7e0",
   "metadata": {},
   "source": [
    "Before applying log transformation, negative values must be dealt with to ensure code is not taking the log of zero or negative values. Will take measures to shift the data to positive before applying log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftPositive(TransformerMixin):\n",
    "    def __init__(self,target_col=None):\n",
    "        self.target_col=target_col\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    # test if float column contains only integer values\n",
    "    def test_type(self,colvector):\n",
    "        return colvector[colvector.notnull()].apply(\n",
    "            lambda x: x.is_integer()).sum() == len(colvector[colvector.notnull()])\n",
    "        \n",
    "    def transform(self, X, y=None, int_amount=1, deci_amount=0.1):\n",
    "        if self.target_col:\n",
    "            X = X.drop(columns=[self.target_col])\n",
    "        \n",
    "        # separate columns into integer vs float-valued\n",
    "        int_cols = X.dtypes[X.dtypes == np.dtype('int64')].index.tolist()\n",
    "        float_cols = X.dtypes[X.dtypes == np.dtype('float64')].index.tolist()\n",
    "\n",
    "        int_with_nans_bool = X[float_cols].apply(self.test_type)\n",
    "        int_with_nans = int_with_nans_bool[int_with_nans_bool].index.tolist()\n",
    "        int_cols.extend(int_with_nans)\n",
    "        float_cols = list(set(float_cols).difference(set(int_with_nans)))\n",
    "\n",
    "        # preserve categorical columns\n",
    "        cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "        # shift integer columns and float columns based on parameters\n",
    "        new_int_df = X[int_cols].apply(lambda x: x - x.min() + int_amount if x.min() <= 0 else x)\n",
    "        new_float_df = X[float_cols].apply(lambda x: x - x.min() + deci_amount if x.min() <= 0 else x)\n",
    "\n",
    "        # merge categorical columns back with numerical\n",
    "        transformed_df = pd.concat([new_int_df, new_float_df, X[cat_cols]], axis=1)\n",
    "        \n",
    "        if self.target_col and self.target_col in y:\n",
    "            transformed_df[self.target_col] = y\n",
    "            \n",
    "        return transformed_df, int_cols, float_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_pos=ShiftPositive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_ck, int_cols, float_cols, cat_cols=shift_pos.fit_transform(imputed_ck)\n",
    "\n",
    "# print('integer cols:', int_cols)\n",
    "# print('float cols:', float_cols)\n",
    "# shifted_ck['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransform(TransformerMixin):\n",
    "    def __init__(self, target_col=None):\n",
    "        self.target_col = target_col\n",
    "    def fit(self,x,y=None):\n",
    "        return self\n",
    "    \n",
    "    # test if feature is normally distributed\n",
    "    def dist_test(self,colvec,test='normal'):\n",
    "        if colvec.dtype.kind not in 'biufc':  # numeric column check\n",
    "            return np.nan\n",
    "        if test == 'normal':\n",
    "            return stats.mstats.normaltest(colvec[colvec.notnull()])[0]\n",
    "        elif test == 'skew':\n",
    "            return stats.mstats.skewtest(colvec[colvec.notnull()])[0]\n",
    "        elif test == 'kurtosis':\n",
    "            return stats.mstats.kurtosistest(colvec[colvec.notnull()])[0]\n",
    "        else:\n",
    "            print('unknown test type')\n",
    "            return\n",
    "        \n",
    "    # applies log transformation to cols identified using dist_test\n",
    "    def transform(self,X,y=None,thresh=5000):\n",
    "        if self.target_col in X.columns:\n",
    "            X = X.drop(columns=[self.target_col])\n",
    "        \n",
    "        # preserve categorical columns\n",
    "        cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "        \n",
    "        # Apply normal test and determine columns to transform\n",
    "        test_results = X.apply(self.dist_test)\n",
    "        to_transform_cols = test_results[test_results > thresh].index.tolist()\n",
    "\n",
    "        # Apply log transform to identified columns\n",
    "        transformed_cols = X[to_transform_cols].apply(lambda x: np.log(x + 1) if test_results[x.name] > thresh else x)  # Shift positive value before transforming\n",
    "        \n",
    "        # Combine with non-transformed columns\n",
    "        unchanged_cols = list(set(X.columns) - set(to_transform_cols))\n",
    "        transformed_df = pd.concat([X[unchanged_cols], transformed_cols], axis=1)\n",
    "\n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae108b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer=LogTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ck=log_transformer.fit_transform(shifted_ck)\n",
    "# print(transformed_ck.head(2))\n",
    "# print('transformed_ck[\"target\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bea202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df,target_col=None):\n",
    "    if target_col and target_col in df.columns:\n",
    "        target=df[target_col]\n",
    "        df = df.drop(columns=[target_col])\n",
    "    else:\n",
    "        target=None\n",
    "    \n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    scaler=StandardScaler()\n",
    "    standardize=scaler.fit_transform(df[num_cols])\n",
    "    standardized_df = pd.DataFrame(standardize, columns=num_cols, index=df.index)\n",
    "    \n",
    "    # combine with non-numerical columns\n",
    "    non_numeric_cols = df.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "    if not non_numeric_cols.empty:\n",
    "        standardized_df = pd.concat([standardized_df, df[non_numeric_cols]], axis=1)\n",
    "    \n",
    "    if target is not None:\n",
    "        standardized_df[target_col] = target\n",
    "    \n",
    "    return standardized_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca13677",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_ck, scaler=standardize(transformed_ck,target_col='target')\n",
    "# standardized_ck['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19b27c",
   "metadata": {},
   "source": [
    "### 2.2 outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f26b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outliers:\n",
    "    def __init__(self,df,out_thresh):\n",
    "        self.df=df\n",
    "        self.out_thresh=out_thresh\n",
    "        self.outlier_counts={}\n",
    "        \n",
    "    def calc_bounds(self,col):\n",
    "        Q1 = self.df[col].quantile(0.25)\n",
    "        Q3 = self.df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        return lower_bound, upper_bound\n",
    "    \n",
    "    def count_outliers(self, col, lower_bound, upper_bound):\n",
    "        return ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()\n",
    "    \n",
    "    def remove_outlier(self):\n",
    "        outlier_df = self.df.copy()\n",
    "        \n",
    "        for col in outlier_df.select_dtypes(include=[np.number]).columns:\n",
    "            lower_bound, upper_bound = self.calc_bounds(col)\n",
    "            outlier_counts = self.count_outliers(col, lower_bound, upper_bound)\n",
    "            self.outlier_counts[col]=outlier_counts\n",
    "            outlier_df = outlier_df[(outlier_df[col] >= lower_bound) | (outlier_df[col] <= upper_bound)]\n",
    "\n",
    "        return outlier_df\n",
    "    \n",
    "    def display_outlier(self):\n",
    "        for col, count in self.outlier_counts.items():\n",
    "            print(f'Number of outliers in {col}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999cc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_ck=Outliers(standardized_ck,out_thresh=3)\n",
    "cleaned_ck = processor_ck.remove_outlier()\n",
    "# processor_ck.display_outlier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7aadd3",
   "metadata": {},
   "source": [
    "### 2.3 Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76681052",
   "metadata": {},
   "source": [
    "Encoding the categories are straightforward since most of them  have under 10 unique values with no specific levels of ordering; so one-hot encoding will be the standard.\n",
    "\n",
    "Exception: There are a few categories with 50+ unique values (states & cities). Will use grouping & one-hot encode - reduce number of categories by mapping states into regions/larger geographic areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afba627",
   "metadata": {},
   "source": [
    "#### 2.3.1 Group & one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78185392",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping = {\n",
    "    'Northeast': ['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'],\n",
    "    'Midwest': ['IN', 'IL', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD'],\n",
    "    'South': ['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'],\n",
    "    'West': ['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA']\n",
    "}\n",
    "\n",
    "flattened_region_mapping = {}\n",
    "for region, states in region_mapping.items():\n",
    "    for state in states:\n",
    "        flattened_region_mapping[state] = region\n",
    "        \n",
    "# flattened_region_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProcessor:\n",
    "    def __init__(self,df,region_mapping):\n",
    "        self.df=df\n",
    "        self.region_mapping=region_mapping\n",
    "        \n",
    "    def create_col(self):\n",
    "        self.df['REGION_0237']=self.df['VAR_0237'].map(self.region_mapping)\n",
    "        self.df['REGION_0274']=self.df['VAR_0274'].map(self.region_mapping)\n",
    "        \n",
    "    def drop_col(self):\n",
    "        self.df.drop(columns=['VAR_0237','VAR_0274','VAR_0342','VAR_0200'], inplace=True)\n",
    "        \n",
    "    def process_df(self):\n",
    "        self.create_col()\n",
    "        self.drop_col()\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8803f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processor=RegionProcessor(cleaned_ck,flattened_region_mapping)\n",
    "grouped_ck=processor.process_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cec2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder:\n",
    "    def __init__(self,df,columns):\n",
    "        self.df=df\n",
    "        self.columns=columns\n",
    "        \n",
    "    def one_hot(self):\n",
    "        self.df=pd.get_dummies(self.df,columns=self.columns)\n",
    "        \n",
    "    def process_df(self):\n",
    "        self.one_hot()\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ee9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cat col\n",
    "categorical_col=grouped_ck.select_dtypes(include=['object']).columns\n",
    "\n",
    "# instantiate class and process\n",
    "encoder=OneHotEncoder(grouped_ck,categorical_col)\n",
    "encoded_ck=encoder.process_df()\n",
    "\n",
    "# print(encoded_ck.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9930055",
   "metadata": {},
   "source": [
    "## 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a6479",
   "metadata": {},
   "source": [
    "### 3.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ef14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train chunk\n",
    "encoded_ck.to_csv('train_ck.csv', index=False)\n",
    "print('train chunk has been saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c70bef",
   "metadata": {},
   "source": [
    "### 3.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92287a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test chunk\n",
    "\n",
    "# can load ~1/6 of dataset so will split into 3 chunks\n",
    "n_chunks=6\n",
    "\n",
    "# total num of rows in OG file\n",
    "n_rows=sum(1 for row in open('test.csv')) -1 # subtract header row\n",
    "\n",
    "chunk_size=n_rows // n_chunks\n",
    "\n",
    "test_chunk = next(pd.read_csv('test.csv', chunksize=chunk_size, low_memory=False))\n",
    "print(f'chunk shape - before : {test_chunk.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55be9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for test chunk\n",
    "\n",
    "# drop col with NA > than threshold (50%)\n",
    "NA_thresh = 0.5\n",
    "# outlier threshold\n",
    "out_thresh=3\n",
    "\n",
    "# define features\n",
    "cat_col=test_chunk.select_dtypes(include='object').columns\n",
    "time_col=['VAR_0073','VAR_0075','VAR_0156','VAR_0157',\n",
    "          'VAR_0158','VAR_0159','VAR_0166','VAR_0167','VAR_0168','VAR_0169',\n",
    "           'VAR_0176','VAR_0177','VAR_0178','VAR_0179','VAR_0204','VAR_0217','VAR_0314','VAR_0531']\n",
    "num_col=test_chunk.select_dtypes(include=np.number).columns.difference(time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d01682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep test\n",
    "\n",
    "processor_ck=Prep(test_chunk,time_col)\n",
    "prepped_test=processor_ck.process_df()\n",
    "\n",
    "print(f'Chunk shape - after: {prepped_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95724e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA test\n",
    "\n",
    "processor_ck=NA(prepped_test,NA_thresh,num_col,cat_col)\n",
    "\n",
    "# initial NA count\n",
    "print(f'Number of NA - before: {np.any(prepped_test.isnull())}')\n",
    "\n",
    "# apply NA function\n",
    "imputed_test = processor_ck.process_na()\n",
    "\n",
    "# post NA count\n",
    "print(f'Number of NA - after: {np.any(imputed_test.isnull())}')\n",
    "print(f'Chunk shape - after: {imputed_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95327dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting positive test chunk\n",
    "shifted_test, int_cols, float_cols, cat_cols=shift_pos.fit_transform(imputed_test)\n",
    "\n",
    "# log transofroming test chunk\n",
    "transformed_test=log_transformer.fit_transform(shifted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized test chunk\n",
    "standardized_test, scaler=standardize(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_test=Outliers(standardized_test,out_thresh=3)\n",
    "cleaned_test = processor_test.remove_outlier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor=RegionProcessor(cleaned_test,flattened_region_mapping)\n",
    "grouped_test=processor.process_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cat col\n",
    "categorical_col=grouped_test.select_dtypes(include=['object']).columns\n",
    "\n",
    "# instantiate class and process\n",
    "encoder=OneHotEncoder(grouped_test,categorical_col)\n",
    "encoded_test=encoder.process_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test chunk\n",
    "encoded_test.to_csv('test_ck.csv', index=False)\n",
    "print('test chunk has been saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
